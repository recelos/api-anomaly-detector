{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitch explicit language detection using traditional models and graph neural network models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "import xgboost as xgb\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "\n",
    "df = pd.read_csv(f'{data_path}/ENGB/ENGB_target.csv')\n",
    "df['partner'] = LabelEncoder().fit_transform(df['partner'])\n",
    "df[['days', 'views']] = StandardScaler().fit_transform(df[['days', 'views']])\n",
    "X = df[['days', 'views', 'partner']]\n",
    "y = df['mature']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=142)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Random Forest classifier with hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "CV_rfc = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5)\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", CV_rfc.best_params_)\n",
    "\n",
    "y_pred = CV_rfc.predict(X_test)\n",
    "rfc_report = classification_report(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "importances = CV_rfc.best_estimator_.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importances = sorted(zip(importances, feature_names), reverse=True)\n",
    "print(\"Feature importances:\")\n",
    "for importance, name in feature_importances:\n",
    "    print(f\"{name}: {importance}\")\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "rf_recall = recall_score(y_test, y_pred)\n",
    "rf_f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1 score: {rf_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = CV_rfc.best_index_\n",
    "cv_results = CV_rfc.cv_results_\n",
    "rf_results = [\n",
    "    cv_results[f'split{i}_test_score'][best_index] for i in range(5)\n",
    "]\n",
    "\n",
    "print(\"Test scores for each fold for the best parameter combination:\")\n",
    "for i, test_score in enumerate(rf_results):\n",
    "    print(f\"Fold {i + 1} test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Gradient boost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.8, 0.9, 1],\n",
    "}\n",
    "\n",
    "xgboost = GridSearchCV(xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "                           param_grid, \n",
    "                           cv=5, \n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)\n",
    "xgboost.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", xgboost.best_params_)\n",
    "\n",
    "best_model = xgboost.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred)\n",
    "xgb_recall = recall_score(y_test, y_pred)\n",
    "xgb_f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {xgb_accuracy}\")\n",
    "print(f\"Recall: {xgb_recall}\")\n",
    "print(f\"F1 score: {xgb_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = xgboost.best_index_\n",
    "cv_results = xgboost.cv_results_\n",
    "xgb_scores = [\n",
    "    cv_results[f'split{i}_test_score'][best_index] for i in range(5)\n",
    "]\n",
    "\n",
    "print(\"Test scores for each fold for the best parameter combination:\")\n",
    "for i, test_score in enumerate(xgb_scores):\n",
    "    print(f\"Fold {i + 1} test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed data in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    data_edges = pd.read_csv(data_path + '/ENGB/ENGB_edges.csv')\n",
    "    data_target = df\n",
    "    with open(data_path + '/ENGB/ENGB_features.json') as f:\n",
    "        node_features_json = json.load(f)\n",
    "    node_features_df = pd.DataFrame.from_dict(node_features_json, orient='index')\n",
    "    node_features_df.index.name = 'id'\n",
    "    node_features_df.reset_index(inplace=True)\n",
    "    return data_edges, data_target, node_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_target, data_edges, node_features_df):\n",
    "    data_target = data_target.drop(columns=['id'])\n",
    "    data_target = data_target.rename(columns={'new_id': 'id'})\n",
    "    data_edges = data_edges.rename(columns={'from': 'from_id', 'to': 'to_id'})\n",
    "    data_target['mature'] = data_target['mature'].astype(int)\n",
    "    data_target['partner'] = data_target['partner'].astype(int)\n",
    "    data_target['days'] = (data_target['days'] - data_target['days'].min()) / (data_target['days'].max() - data_target['days'].min())\n",
    "    data_target['views'] = (data_target['views'] - data_target['views'].min()) / (data_target['views'].max() - data_target['views'].min())\n",
    "    node_features = torch.tensor(data_target.drop(columns=['id']).values, dtype=torch.float)\n",
    "    data_target['id'] = data_target['id'].astype('int64')\n",
    "    node_features_df['id'] = node_features_df['id'].astype('int64')\n",
    "    data_target = pd.merge(data_target, node_features_df, on='id')\n",
    "    data_target = data_target.fillna(0)\n",
    "    return data_target, data_edges, node_features\n",
    "\n",
    "def prepare_data(data_target, data_edges, node_features):\n",
    "    edge_index = torch.tensor(data_edges.values, dtype=torch.long).t().contiguous()\n",
    "    edge_index = torch.cat([edge_index, edge_index[[1, 0]]], dim=1)\n",
    "    node_ids = data_target['id']\n",
    "    labels = torch.tensor(data_target['mature'].values, dtype=torch.long)\n",
    "    train_indices, test_indices = train_test_split(range(len(node_ids)), test_size=0.20, stratify=labels)\n",
    "    train_mask = torch.zeros(len(node_ids), dtype=torch.bool).scatter_(0, torch.tensor(train_indices), True)\n",
    "    test_mask = torch.zeros(len(node_ids), dtype=torch.bool).scatter_(0, torch.tensor(test_indices), True)\n",
    "    data = Data(x=node_features, edge_index=edge_index, y=labels, train_mask=train_mask, test_mask=test_mask)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(node_features.shape[1], 32)\n",
    "        self.conv2 = GCNConv(32, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, train_mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data)\n",
    "        preds = logits[mask].max(1)[1]\n",
    "        labels = data.y[mask]\n",
    "        acc = accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1 = f1_score(labels.cpu(), preds.cpu(), average='macro')\n",
    "        recall = recall_score(labels.cpu(), preds.cpu(), average='macro')\n",
    "    return acc, f1, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_edges, data_target, node_features_df = load_data(df)\n",
    "data_target, data_edges, node_features = preprocess_data(data_target, data_edges, node_features_df)\n",
    "data = prepare_data(data_target, data_edges, node_features)\n",
    "all_loss_values = []\n",
    "all_accuracy_values = []\n",
    "all_f1_values = []\n",
    "all_recall_values = []\n",
    "EPOCHS = 2000\n",
    "history = []\n",
    "best_acc = 0\n",
    "PATIENCE = 25\n",
    "gcn_cv_accuracies = []\n",
    "gcn_cv_f1s = []\n",
    "gcn_cv_recalls = []\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for train_index, test_index in skf.split(data.x, data.y):\n",
    "    train_index = torch.tensor(train_index, dtype=torch.int64)\n",
    "    test_index = torch.tensor(test_index, dtype=torch.int64)\n",
    "    train_mask = torch.zeros(len(data.y), dtype=torch.bool).scatter_(0, train_index, True)\n",
    "    test_mask = torch.zeros(len(data.y), dtype=torch.bool).scatter_(0, test_index, True)\n",
    "\n",
    "    model = GCN(data.x).to(device)\n",
    "    data = data.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    epochs = 2000\n",
    "    loss_values = []\n",
    "    accuracy_values = []\n",
    "    f1_values = []\n",
    "    recall_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = train(model, data, optimizer, criterion, train_mask)\n",
    "        acc, f1, recall = test(model, data, test_mask)\n",
    "        loss_values.append(loss)\n",
    "        accuracy_values.append(acc)\n",
    "        f1_values.append(f1)\n",
    "        recall_values.append(recall)\n",
    "        print(f'Epoch: {epoch + 1:03d}, Loss: {loss:.4f}, Acc: {acc:.4f}')\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    if epochs_no_improve == PATIENCE:\n",
    "        print(f\"Accuracy has not improved in the last {PATIENCE} epochs, stopping training.\")\n",
    "        break\n",
    "\n",
    "    all_loss_values.append(loss_values)\n",
    "    all_accuracy_values.append(accuracy_values)\n",
    "    all_f1_values.append(f1_values)\n",
    "    all_recall_values.append(recall_values)\n",
    "    final_acc, final_f1, final_recall = test(model, data, test_mask)\n",
    "    gcn_cv_accuracies.append(final_acc)\n",
    "    gcn_cv_f1s.append(final_f1)\n",
    "    gcn_cv_recalls.append(final_recall)\n",
    "\n",
    "avg_loss_values = [sum(x)/len(x) for x in zip(*all_loss_values)]\n",
    "avg_accuracy_values = [sum(x)/len(x) for x in zip(*all_accuracy_values)]\n",
    "avg_f1_values = [sum(x)/len(x) for x in zip(*all_f1_values)]\n",
    "avg_recall_values = [sum(x)/len(x) for x in zip(*all_recall_values)]\n",
    "\n",
    "print(\"Average Loss: \", avg_loss_values[-1])\n",
    "print(\"Average Accuracy: \", avg_accuracy_values[-1])\n",
    "print(\"Average F1 Score: \", avg_f1_values[-1])\n",
    "print(\"Average Recall: \", avg_recall_values[-1])\n",
    "print(\"Cross validation accuracies: \", gcn_cv_accuracies)\n",
    "print(\"Cross validation F1 scores: \", gcn_cv_f1s)\n",
    "print(\"Cross validation recalls: \", gcn_cv_recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gnn_embeddings = model(data).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine original features with GNN embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_embeddings_df = pd.DataFrame(gnn_embeddings, index=data_target['id'], columns=[f'embedding_{i}' for i in range(gnn_embeddings.shape[1])])\n",
    "\n",
    "combined_df = pd.merge(df, gnn_embeddings_df, left_on='new_id', right_index=True)\n",
    "\n",
    "X_combined = combined_df.drop(columns=['mature'])\n",
    "y_combined = combined_df['mature']\n",
    "\n",
    "X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, y_combined, test_size=0.3, random_state=142)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Gradient Boost on combined feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "print(\"Best parameters for combined model:\", xgboost.best_params_)\n",
    "\n",
    "best_xgb_combined = xgboost.best_estimator_\n",
    "y_pred_combined = best_xgb_combined.predict(X_test_combined)\n",
    "\n",
    "print(classification_report(y_test_combined, y_pred_combined))\n",
    "print(confusion_matrix(y_test_combined, y_pred_combined))\n",
    "\n",
    "hybrid_accuracy = accuracy_score(y_test_combined, y_pred_combined)\n",
    "hybrid_recall = recall_score(y_test_combined, y_pred_combined)\n",
    "hybrid_f1 = f1_score(y_test_combined, y_pred_combined)\n",
    "print(f\"Combined Model Accuracy after hyperparameter tuning: {hybrid_accuracy}\")\n",
    "print(f\"Recall: {hybrid_recall}\")\n",
    "print(f\"F1 score: {hybrid_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = xgboost.best_index_\n",
    "cv_results = xgboost.cv_results_\n",
    "hybrid_scores = [\n",
    "    cv_results[f'split{i}_test_score'][best_index] for i in range(5)\n",
    "]\n",
    "\n",
    "print(\"Test scores for each fold for the best parameter combination:\")\n",
    "for i, test_score in enumerate(hybrid_scores):\n",
    "    print(f\"Fold {i + 1} test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_values, label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over time (for GCN)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracy_values, label='GCN Test Accuracy')\n",
    "plt.axhline(y=rf_accuracy, color='green', linestyle='--', label='Random Forest Test Accuracy')\n",
    "plt.axhline(y=xgb_accuracy, color='red', linestyle='--', label='XGBoost Test Accuracy')\n",
    "plt.axhline(y=hybrid_accuracy, color='blue', linestyle='--', label='Hybrid Model Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over time')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "recall_heights = [rf_recall, xgb_recall, avg_recall_values[-1], hybrid_recall]\n",
    "bars = ('Random Forest', 'Gradient Boost', 'GCN', 'Hybrid model')\n",
    "y_pos = np.arange(len(recall_heights))\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.bar(y_pos, recall_heights)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Recall scores', fontsize=16, color='#323232')\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "f1_heights = [rf_f1, xgb_f1, avg_f1_values[-1], hybrid_f1]\n",
    "y_pos = np.arange(len(f1_heights))\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.bar(y_pos, f1_heights)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('F1 scores', fontsize=16, color='#323232')\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "acc_heights = [rf_accuracy, xgb_accuracy, avg_accuracy_values[-1], hybrid_accuracy]\n",
    "y_pos = np.arange(len(acc_heights))\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.bar(y_pos, acc_heights)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Accuracies', fontsize=16, color='#323232')\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, f_oneway, friedmanchisquare\n",
    "\n",
    "t_statistic, t_p_value = ttest_rel(rf_results, xgb_scores)\n",
    "print(f\"t-test for Random forest and XGBoost: t-statistic: {t_statistic}, p-value: {t_p_value}\")\n",
    "\n",
    "t_statistic, t_p_value = ttest_rel(rf_results, gcn_cv_accuracies)\n",
    "print(f\"t-test for Random forest and GCN: t-statistic: {t_statistic}, p-value: {t_p_value}\")\n",
    "\n",
    "t_statistic, t_p_value = ttest_rel(rf_results, hybrid_scores)\n",
    "print(f\"t-test for Random forest and hybrid model: t-statistic: {t_statistic}, p-value: {t_p_value}\")\n",
    "\n",
    "t_statistic, t_p_value = ttest_rel(xgb_scores, gcn_cv_accuracies)\n",
    "print(f\"t-test for XGBoost and GCN: t-statistic: {t_statistic}, p-value: {t_p_value}\")\n",
    "\n",
    "t_statistic, t_p_value = ttest_rel(xgb_scores, hybrid_scores)\n",
    "print(f\"t-test for XGBoost and hybrid model: t-statistic: {t_statistic}, p-value: {t_p_value}\")\n",
    "\n",
    "t_statistic, t_p_value = ttest_rel(gcn_cv_accuracies, hybrid_scores)\n",
    "print(f\"t-test for GCN and hybrid model: t-statistic: {t_statistic}, p-value: {t_p_value}\")\n",
    "\n",
    "\n",
    "anova_f_statistic, anova_p_value = f_oneway(rf_results, xgb_scores, gcn_cv_accuracies, hybrid_scores)\n",
    "print(f\"ANOVA: F-statistic: {anova_f_statistic}, p-value: {anova_p_value}\")\n",
    "\n",
    "friedman_statistic, friedman_p_value = friedmanchisquare(rf_results, xgb_scores, gcn_cv_accuracies, hybrid_scores)\n",
    "print(f\"Friedman test: statistic: {friedman_statistic}, p-value: {friedman_p_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

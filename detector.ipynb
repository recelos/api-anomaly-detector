{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI API anomaly detector using graph neural networks models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import ToDense\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import DenseGCNConv\n",
    "import torch_geometric.utils\n",
    "from torch_geometric.data import Batch, Data\n",
    "from torch.nn.functional import pad\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 256)\n",
    "        self.conv2 = GCNConv(256, 256)\n",
    "        self.conv3 = GCNConv(256, num_classes)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.dropout(F.relu(self.conv1(x, edge_index)))\n",
    "        x = self.dropout(F.relu(self.conv2(x, edge_index)))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.squeeze(0)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "csv_path = f'{data_path}\\supervised_dataset.csv'\n",
    "json_path = f'{data_path}\\supervised_call_graphs.json'\n",
    "\n",
    "data_frame = pd.read_csv(csv_path)\n",
    "print(f\"Loaded {len(data_frame)} records from CSV file.\")\n",
    "with open(json_path, 'r') as json_data:\n",
    "  graph_data = json.load(json_data)\n",
    "print(f\"Loaded {len(graph_data)} records from JSON file.\")\n",
    "\n",
    "print(\"Dataframe columns:\", data_frame.columns)\n",
    "print(\"Graph data keys:\", graph_data[0].keys())\n",
    "#print(\"Graph data sample:\", graph_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mappings from IDs to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = {item['_id']: (0 if item['classification'] == 'normal' else 1) for item in data_frame.to_dict('records')}\n",
    "\n",
    "class_counts = pd.Series([id_to_label[_id] for _id in id_to_label]).value_counts() #check the distribution of the classes\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_objects(graph_data, id_to_label):\n",
    "    graphs = []\n",
    "    for item in graph_data:\n",
    "        _id = item['_id']\n",
    "        \n",
    "        # Process to ensure edge indices are numeric and consecutive\n",
    "        node_ids = {}\n",
    "        edge_indices = []\n",
    "        for edge in item['call_graph']:\n",
    "            from_id = edge['fromId']\n",
    "            to_id = edge['toId']\n",
    "            \n",
    "            if from_id not in node_ids:\n",
    "                node_ids[from_id] = len(node_ids)\n",
    "            if to_id not in node_ids:\n",
    "                node_ids[to_id] = len(node_ids)\n",
    "            \n",
    "            edge_indices.append([node_ids[from_id], node_ids[to_id]])\n",
    "        \n",
    "        # Convert edge_indices list to a tensor\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous() if edge_indices else torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "        # Number of nodes and feature matrix\n",
    "        num_nodes = len(node_ids)\n",
    "        num_features = 10  # Choose an appropriate number of features\n",
    "        x = torch.zeros(num_nodes, num_features)  # Initialize feature matrix with zeros\n",
    "        for node_id, i in node_ids.items():\n",
    "            x[i, i % num_features] = 1  # One-hot encoding of node index\n",
    "        \n",
    "        # Get the label and repeat it for each node\n",
    "        label = torch.tensor([id_to_label[_id]] * num_nodes, dtype=torch.long)\n",
    "\n",
    "        graphs.append(Data(x=x, edge_index=edge_index, y=label, num_nodes=num_nodes))\n",
    "\n",
    "    return graphs\n",
    "\n",
    "def collate(data_list):\n",
    "    return Batch.from_data_list(data_list)\n",
    "\n",
    "def oversample_minority(graphs):\n",
    "    # Count the number of instances for each class\n",
    "    class_counts = pd.Series([graph.y[0].item() for graph in graphs if graph.y.nelement() > 0]).value_counts()\n",
    "    \n",
    "    # Identify the minority class\n",
    "    minority_class = class_counts.idxmin()\n",
    "    \n",
    "    # Calculate the difference between the majority and minority class\n",
    "    diff = class_counts.max() - class_counts.min()\n",
    "    \n",
    "    # Get all graphs of the minority class\n",
    "    minority_graphs = [graph for graph in graphs if graph.y.nelement() > 0 and graph.y[0].item() == minority_class]\n",
    "    \n",
    "    # Duplicate the minority graphs until the classes are balanced\n",
    "    duplicate_graphs = random.choices(minority_graphs, k=diff)\n",
    "    \n",
    "    # Add the duplicated graphs to the original list\n",
    "    graphs += duplicate_graphs\n",
    "\n",
    "    return graphs\n",
    "\n",
    "def print_label_distribution(graphs):\n",
    "    # Count the number of instances for each class\n",
    "    class_counts = pd.Series([graph.y[0].item() for graph in graphs if graph.y.nelement() > 0]).value_counts()\n",
    "    \n",
    "    print(class_counts)\n",
    "\n",
    "def data_to_networkx(data):\n",
    "    G = nx.Graph()\n",
    "    edge_index = data.edge_index.t().numpy()\n",
    "    num_nodes = data.num_nodes\n",
    "    for i in range(num_nodes):\n",
    "        G.add_node(i, features=data.x[i].numpy())\n",
    "    G.add_edges_from(edge_index)\n",
    "    return G\n",
    "\n",
    "def visualize_graphs(graphs):\n",
    "    num_graphs = len(graphs)\n",
    "    cols = int(num_graphs**0.5) + 1\n",
    "    rows = (num_graphs + cols - 1) // cols\n",
    "\n",
    "    _, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
    "    axes = axes.flatten() if num_graphs > 1 else [axes]\n",
    "\n",
    "    for ax, graph_data_object in zip(axes, graphs):\n",
    "        nx_graph = data_to_networkx(graph_data_object)\n",
    "        node_labels = {node: f'Node {node}\\nLabel {graph_data_object.y[node].item()}' for node in nx_graph.nodes}\n",
    "         \n",
    "        pos = nx.spring_layout(nx_graph, seed=42)\n",
    "        nx.draw(nx_graph, pos, ax=ax, with_labels=True, node_color='skyblue', node_size=700, font_weight='bold', font_color='darkred', labels=node_labels)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = create_graph_objects(graph_data, id_to_label)\n",
    "graphs = oversample_minority(graphs)\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "\n",
    "print(f\"Created {len(graphs)} graph objects.\")\n",
    "print(\"Graph object sample:\", graphs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the graphs into two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_class_0 = [graph for graph in graphs if graph.y.nelement() > 0 and graph.y[0].item() == 0]\n",
    "graphs_class_1 = [graph for graph in graphs if graph.y.nelement() > 0 and graph.y[0].item() == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_graphs(graphs_class_0[:6])\n",
    "visualize_graphs(graphs_class_1[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a train-test split on each class separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs_class_0, val_graphs_class_0 = train_test_split(graphs_class_0, test_size=0.20, random_state=42)\n",
    "train_graphs_class_1, val_graphs_class_1 = train_test_split(graphs_class_1, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs = train_graphs_class_0 + train_graphs_class_1\n",
    "val_graphs = val_graphs_class_0 + val_graphs_class_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_graphs)\n",
    "random.shuffle(val_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_graphs, batch_size=32, shuffle=False, collate_fn=collate)\n",
    "print(\"Label distribution in all graphs:\")\n",
    "print_label_distribution(graphs)\n",
    "\n",
    "print(\"Label distribution in training graphs:\")\n",
    "print_label_distribution(train_graphs)\n",
    "\n",
    "print(\"Label distribution in validation graphs:\")\n",
    "print_label_distribution(val_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model, optimizer, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = GCN(num_features=graphs[0].num_node_features, num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Decrease learning rate\n",
    "criterion = F.nll_loss\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in val_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(val_dataset)\n",
    "\n",
    "def print_predictions():\n",
    "    model.eval()\n",
    "    for data in val_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        print(f'Predictions: {pred}')\n",
    "        print(f'Actual labels: {data.y}')\n",
    "\n",
    "\n",
    "val_dataset = val_graphs\n",
    "\n",
    "for epoch in range(10):\n",
    "    train()\n",
    "    val_acc = validate()\n",
    "    print(f'Epoch: {epoch}, Validation Accuracy: {val_acc}')\n",
    "\n",
    "print_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
